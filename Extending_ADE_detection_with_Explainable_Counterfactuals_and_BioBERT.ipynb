{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Project Information**\n",
        "## **Authors: Shrey Patel (sp2675), Abhishek Jani (aj1121), Mustafa Adil (ma2398)**\n",
        "- Note:\n",
        "This code was developed and tested using Google Colab.For best results and to ensure all dependencies are handled correctly,it is recommended to upload the corresponding .ipynb file to Google Colab and run the cells sequentially.\n",
        "\n"
      ],
      "metadata": {
        "id": "eObu34ezmJXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup and Installs**"
      ],
      "metadata": {
        "id": "HIJwb4S90Pkn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d8R6OlXwVJ5"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade numpy==1.26.4 # Specific numpy version if required by dependencies\n",
        "# !pip install --upgrade thinc==8.3.5 # Specific thinc version if required by dependencies\n",
        "!pip install --upgrade pandas scikit-learn transformers datasets textattack requests tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm # for progress bars\n",
        "\n",
        "# Import components from libraries\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoConfig\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper # For TextAttack integration\n",
        "tqdm.pandas()\n",
        "print(\"libraries imported\")\n",
        "\n",
        "gpu_available = torch.cuda.is_available()\n",
        "if gpu_available:\n",
        "  print(\"gpu is available\")\n",
        "  DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"No gpu, using cpu\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration**"
      ],
      "metadata": {
        "id": "qLhtmOpx0uVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Note: Seeding for reproducibility is often done here, but currently commented out.\n",
        "\n",
        "Uncomment these lines if you need strict reproducibility across runs.\n",
        "\n",
        "np.random.seed(CONFIG[\"random_seed\"])\n",
        "torch.manual_seed(CONFIG[\"random_seed\"])\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(CONFIG[\"random_seed\"])\n",
        "```"
      ],
      "metadata": {
        "id": "XvgBu4iRkraM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    #dataset\n",
        "    \"classification_data_path\": \"hf://datasets/ade-benchmark-corpus/ade_corpus_v2/Ade_corpus_v2_classification/train-00000-of-00001.parquet\",\n",
        "    \"relation_data_path\": \"hf://datasets/ade-benchmark-corpus/ade_corpus_v2/Ade_corpus_v2_drug_ade_relation/train-00000-of-00001.parquet\",\n",
        "    \"extracted_drugs_file\": \"extracted_drug_names.json\",\n",
        "    \"brand_mapping_file\": \"full_drug_brand_mapping.json\",\n",
        "    \"modified_data_file\": \"classification_data_with_brands.parquet\",\n",
        "    #model\n",
        "    \"base_model_name\": \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    \"finetuned_model_dir\": \"./trained_model\",\n",
        "    \"tokenizer_max_length\": 128,\n",
        "    \"num_labels\": 2,\n",
        "\n",
        "    \"training_output_dir\": \"./results_quick\",\n",
        "    \"num_train_epochs\": 10,\n",
        "    \"per_device_train_batch_size\": 16,\n",
        "    \"per_device_eval_batch_size\": 16,\n",
        "    \"save_strategy_finetune\": \"epoch\",\n",
        "    \"logging_steps\": 200,\n",
        "    \"report_to\": \"none\",\n",
        "    \"dataloader_num_workers\": 2,\n",
        "    \"fp16_training\": torch.cuda.is_available(),\n",
        "\n",
        "    \"ade_label\": 1,\n",
        "    \"test_split_size\": 0.2,\n",
        "    \"random_seed\": 42,\n",
        "    #api\n",
        "    \"api_delay_seconds\": 0.2,\n",
        "    \"api_timeout_seconds\": 10,\n",
        "    #ouput files\n",
        "    \"masking_flip_ade_file\": \"masking_flip_examples_ade_only.json\",\n",
        "    \"masking_nonflip_ade_file\": \"masking_non_flip_examples_ade_only.json\",\n",
        "    \"masking_flip_non_ade_file\": \"output/masking_non_ade_flips.json\",\n",
        "    \"masking_nonflip_non_ade_file\": \"output/masking_non_ade_nonflips.json\",\n",
        "    \"brand_flip_all_file\": \"brand_flip_examples.json\",\n",
        "    \"brand_nonflip_all_file\": \"brand_nonflip_examples.json\",\n",
        "    \"brand_flip_ade_file\": \"brand_flip_examples_ade_only.json\",\n",
        "    \"brand_nonflip_ade_file\": \"brand_nonflip_examples_ade_only.json\",\n",
        "}\n",
        "\n",
        "# np.random.seed(CONFIG[\"random_seed\"])\n",
        "# torch.manual_seed(CONFIG[\"random_seed\"])\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed_all(CONFIG[\"random_seed\"])"
      ],
      "metadata": {
        "id": "lyVviEauwsvo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility Functions**"
      ],
      "metadata": {
        "id": "liRE2PLi4-5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label(text, model_wrapper):\n",
        "    if not model_wrapper:\n",
        "         raise ValueError(\"Model wrapper is not available.\")\n",
        "    try:\n",
        "        # Ensuring that the text is a non-empty string\n",
        "        if not isinstance(text, str): text = str(text)\n",
        "        if not text or text.strip() == \"\":\n",
        "            return 0 # default  for empty or invalid input\n",
        "        outputs = model_wrapper([text])\n",
        "        logits = outputs[0]\n",
        "        if isinstance(logits, torch.Tensor):\n",
        "            logits = logits.cpu().detach().numpy()\n",
        "        # for handling NaN/inf in logits if model outputs them\n",
        "        if not np.all(np.isfinite(logits)):\n",
        "             print(f\"Warning: Non-finite logits encountered for text: '{text[:50]}...'. Returning default (0).\")\n",
        "             return 0\n",
        "        return np.argmax(logits)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction for text: '{text[:50]}...': {e}\")\n",
        "        return 0 # on error return default"
      ],
      "metadata": {
        "id": "NX1YbzTJw5rt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_json(data, filename):\n",
        "    try:\n",
        "        output_dir = os.path.dirname(filename)\n",
        "        if output_dir and not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        with open(filename, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Had an error saving data to {filename}: {e}\")"
      ],
      "metadata": {
        "id": "LkslzBPml3o7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"file not found {filename}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"error decoding {filename}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"erroe reading {filename}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "E1DgwxnDl6mR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_create_dir(directory_path):# func checks if a dir exists, if it does not exist then creates it.\n",
        "    if directory_path and not os.path.exists(directory_path):\n",
        "        print(f\"Creating directory {directory_path}\")\n",
        "        os.makedirs(directory_path)"
      ],
      "metadata": {
        "id": "OmevYgLel9Ry"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Preprocessing Functions**"
      ],
      "metadata": {
        "id": "owgiOGUh5EyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_parquet_data(path):\n",
        "    try:\n",
        "        df = pd.read_parquet(path)\n",
        "        print(f\"loaded {len(df)} rows from {path}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading parquet file from {path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Tbo10RfUw70K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_drug_names(df_relation, drug_col='drug'):\n",
        "    if df_relation is None or drug_col not in df_relation.columns:\n",
        "        print(f\"Error in drug data '{drug_col}'.\")\n",
        "        return []\n",
        "    unique_drugs = set()\n",
        "    for drugs_entry in df_relation[drug_col].dropna():\n",
        "        for drug in str(drugs_entry).split(','):\n",
        "            drug_clean = drug.strip().lower()\n",
        "            if drug_clean and len(drug_clean) > 1: # Adding basic validation\n",
        "                unique_drugs.add(drug_clean)\n",
        "    sorted_drugs = sorted(list(unique_drugs))\n",
        "    print(f\"there are {len(sorted_drugs)} unique drug names\")\n",
        "    return sorted_drugs"
      ],
      "metadata": {
        "id": "hcY46Ss0mBRy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function_hf(examples, tokenizer, max_length, text_col=\"text\"):\n",
        "    return tokenizer(\n",
        "        examples[text_col],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length\n",
        "    )"
      ],
      "metadata": {
        "id": "gN_xweuNmGOw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_hf_dataset(df, tokenizer, config, text_col=\"text\", label_col=\"label\"):\n",
        "    if df is None or text_col not in df.columns or label_col not in df.columns:\n",
        "         print(\"Error in dataset creation\")\n",
        "         return None, None # Return None for both training and test\n",
        "\n",
        "    dataset = Dataset.from_pandas(df[[text_col, label_col]])\n",
        "\n",
        "    print(\"Splitting dataset\")\n",
        "    split = dataset.train_test_split(test_size=config[\"test_split_size\"], seed=config[\"random_seed\"])\n",
        "    train_dataset = split[\"train\"]\n",
        "    test_dataset = split[\"test\"]\n",
        "\n",
        "    #tokenizing\n",
        "    train_dataset = train_dataset.map(\n",
        "        lambda examples: tokenize_function_hf(examples, tokenizer, config[\"tokenizer_max_length\"], text_col),\n",
        "        batched=True\n",
        "    )\n",
        "    test_dataset = test_dataset.map(\n",
        "        lambda examples: tokenize_function_hf(examples, tokenizer, config[\"tokenizer_max_length\"], text_col),\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_col])\n",
        "        test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_col])\n",
        "        train_dataset = train_dataset.rename_column(label_col, \"labels\")\n",
        "        test_dataset = test_dataset.rename_column(label_col, \"labels\")\n",
        "    except Exception as e:\n",
        "         print(f\"error{e}\")\n",
        "         return None, None\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "MrYLwGvumJWY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Functions**"
      ],
      "metadata": {
        "id": "N405Rmsh5JSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokenizer_and_model(model_name_or_path, num_labels):\n",
        "    try:\n",
        "        print(f\"loading tokenizer from {model_name_or_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "        print(f\"loading model from {model_name_or_path}\")\n",
        "        model_config = AutoConfig.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            config=model_config\n",
        "        )\n",
        "        model.to(DEVICE)\n",
        "        print(\"loaded successfully\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading{model_name_or_path}: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "4uXREyQIw96D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_wrapper(model, tokenizer):\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"Error, no model or tokenizer\")\n",
        "        return None\n",
        "    try:\n",
        "        model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
        "        print(\"Model is wrapped successfully.\")\n",
        "        return model_wrapper\n",
        "    except Exception as e:\n",
        "        print(f\"Error in wrapping model: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "RbX9FmVtmRFo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics_hf(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='macro')\n",
        "    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
      ],
      "metadata": {
        "id": "w17K7eTVmUiO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, tokenizer, train_dataset, eval_dataset, config):\n",
        "    if model is None or tokenizer is None or train_dataset is None or eval_dataset is None:\n",
        "        print(\"Error in training\")\n",
        "        return None\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config[\"training_output_dir\"],\n",
        "        num_train_epochs=config[\"num_train_epochs\"],\n",
        "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n",
        "        save_strategy=config[\"save_strategy_finetune\"],\n",
        "        logging_steps=config[\"logging_steps\"],\n",
        "        report_to=config[\"report_to\"],\n",
        "        fp16=config[\"fp16_training\"],\n",
        "        dataloader_num_workers=config[\"dataloader_num_workers\"],\n",
        "        eval_strategy=\"epoch\" if config[\"save_strategy_finetune\"] != \"no\" else \"no\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True if config[\"save_strategy_finetune\"] != \"no\" else False,\n",
        "        metric_for_best_model=\"f1\" if config[\"save_strategy_finetune\"] != \"no\" else None,\n",
        "        seed=config[\"random_seed\"]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics_hf\n",
        "    )\n",
        "\n",
        "    print(\"Starting model finetuning\")\n",
        "    train_result = trainer.train()\n",
        "    print(\"finished model finetuning\")\n",
        "\n",
        "    print(\"test set evaluation\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation metrics:\", eval_results)\n",
        "\n",
        "    save_directory = config[\"finetuned_model_dir\"]\n",
        "    check_and_create_dir(save_directory)\n",
        "    print(f\"Saving model and tokenizer to: {save_directory}\")\n",
        "    trainer.save_model(save_directory)\n",
        "\n",
        "    print(\"Model training and saving complete.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "Xwx-lwCOmYDm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis Functions**"
      ],
      "metadata": {
        "id": "dp-HItio5ORD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_drug_names(text, drug_names_list, mask_token='[MASK]'):\n",
        "    if not isinstance(text, str) or not drug_names_list:\n",
        "        return text, False\n",
        "    masked_text = text\n",
        "    found = False\n",
        "    sorted_drug_names = sorted(drug_names_list, key=len, reverse=True)\n",
        "\n",
        "    for drug in sorted_drug_names:\n",
        "        if not drug: continue\n",
        "        try:\n",
        "            #using \\b for word boundaries\n",
        "            pattern = re.compile(r'\\b' + re.escape(drug) + r'\\b', flags=re.IGNORECASE)\n",
        "            temp_masked_text = pattern.sub(mask_token, masked_text)\n",
        "            if temp_masked_text != masked_text:\n",
        "                found = True\n",
        "                masked_text = temp_masked_text\n",
        "        except re.error as e:\n",
        "            continue\n",
        "    return masked_text, found"
      ],
      "metadata": {
        "id": "b3ab1UuExDUq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_brand_replacement(mapping_dict):\n",
        "    if not mapping_dict:\n",
        "        return {}, [], {}\n",
        "    # Ensuring the keys are lowercase for consistent matching\n",
        "    lower_mapping_dict = {k.lower(): v for k, v in mapping_dict.items()}\n",
        "    compiled_patterns = {\n",
        "        generic_lower: re.compile(r'\\b' + re.escape(generic_lower) + r'\\b', flags=re.IGNORECASE)\n",
        "        for generic_lower in lower_mapping_dict.keys()\n",
        "    }\n",
        "    sorted_generic_keys = sorted(lower_mapping_dict.keys(), key=len, reverse=True)\n",
        "    return lower_mapping_dict, compiled_patterns, sorted_generic_keys"
      ],
      "metadata": {
        "id": "7yYrY1Fumfel"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_generic_with_brand(text, lower_mapping_dict, compiled_patterns, sorted_keys):\n",
        "    if not isinstance(text, str) or not lower_mapping_dict:\n",
        "        return text, False\n",
        "\n",
        "    current_text = text\n",
        "    replaced = False\n",
        "    for generic_lower in sorted_keys:\n",
        "        pattern = compiled_patterns.get(generic_lower)\n",
        "        brand_name = lower_mapping_dict.get(generic_lower)\n",
        "        if pattern and brand_name:\n",
        "            new_text = pattern.sub(brand_name, current_text)\n",
        "            if new_text != current_text:\n",
        "                replaced = True\n",
        "                current_text = new_text\n",
        "    return current_text, replaced"
      ],
      "metadata": {
        "id": "fPwIwS0TmjP9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_flip_rate(df, text_col1, text_col2, model_wrapper, description=\"Processing\"):\n",
        "    if df is None or df.empty or model_wrapper is None:\n",
        "        print(f\"'{description}': invalid inputs.\")\n",
        "        return 0.0, [], []\n",
        "\n",
        "    flip_count = 0\n",
        "    total_processed = 0\n",
        "    flip_examples = []\n",
        "    non_flip_examples = []\n",
        "\n",
        "    print(f\"\\nCalculating flip rate for: {description}\")\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=description):\n",
        "        text1 = row[text_col1]\n",
        "        text2 = row[text_col2]\n",
        "\n",
        "        #Basic validation\n",
        "        if not isinstance(text1, str) or not isinstance(text2, str) or not text1 or not text2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pred1 = predict_label(text1, model_wrapper)\n",
        "            pred2 = predict_label(text2, model_wrapper)\n",
        "            total_processed += 1\n",
        "\n",
        "            example_data = {\n",
        "                'index': index,\n",
        "                text_col1: text1,\n",
        "                text_col2: text2,\n",
        "                'prediction1': int(pred1),\n",
        "                'prediction2': int(pred2)\n",
        "            }\n",
        "            if 'label' in row: example_data['original_label'] = int(row['label'])\n",
        "            if pred1 != pred2:\n",
        "                flip_count += 1\n",
        "                flip_examples.append(example_data)\n",
        "            else:\n",
        "                non_flip_examples.append(example_data)\n",
        "        except Exception as e:\n",
        "            print(f\"error in flip calculation for index {index} in {description}: {e}\")\n",
        "\n",
        "    flip_rate = flip_count / total_processed if total_processed > 0 else 0.0\n",
        "    print(f\"Results ({description}) ---\")\n",
        "    print(f\"Total sentences processed: {total_processed}\")\n",
        "    print(f\"sentences where prediction FLIPPED: {flip_count}\")\n",
        "    print(f\"sentences where prediction DID NOT FLIP: {len(non_flip_examples)}\")\n",
        "    print(f\"Flip Rate: {flip_rate:.4f}\")\n",
        "\n",
        "    return flip_rate, flip_examples, non_flip_examples"
      ],
      "metadata": {
        "id": "8rrCVemzmmTc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **API Functions (RxNorm)**"
      ],
      "metadata": {
        "id": "XxuLvEfD5TL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rxcui(generic_name, config):\n",
        "    base_url = \"https://rxnav.nlm.nih.gov/REST/rxcui.json\"\n",
        "    params = {'name': generic_name, 'search': 2} #Approx match\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, timeout=config[\"api_timeout_seconds\"])\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if 'idGroup' in data and 'rxnormId' in data['idGroup'] and data['idGroup']['rxnormId']:\n",
        "            return data['idGroup']['rxnormId'][0] #Return first RxCUI\n",
        "        return None\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Timeout fetching RxCUI for '{generic_name}'\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API request failed for RxCUI lookup '{generic_name}': {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in get_rxcui for '{generic_name}': {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "Q7H_I0VfxFxr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_brand_names(rxcui, config):\n",
        "    if not rxcui: return []\n",
        "    base_url = f\"https://rxnav.nlm.nih.gov/REST/rxcui/{rxcui}/related.json\"\n",
        "    params = {'tty': 'BN'}\n",
        "    brands = []\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params, timeout=config[\"api_timeout_seconds\"])\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if ('relatedGroup' in data and 'conceptGroup' in data['relatedGroup']):\n",
        "            for group in data['relatedGroup']['conceptGroup']:\n",
        "                if group.get('tty') == 'BN' and 'conceptProperties' in group:\n",
        "                    for prop in group['conceptProperties']:\n",
        "                        if 'name' in prop: brands.append(prop['name'])\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        if response.status_code != 404:\n",
        "             print(f\"HTTP error fetching brands for RxCUI {rxcui}: {http_err}\")\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Timeout\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API request failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"error\")\n",
        "    return brands"
      ],
      "metadata": {
        "id": "O3A9yGO7mrEc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_brand_mappings(generic_names_list, config):\n",
        "    if not generic_names_list: return []\n",
        "    all_mappings = []\n",
        "    processed_count = 0\n",
        "    found_count = 0\n",
        "    print(f\"API lookups for {len(generic_names_list)} names\")\n",
        "\n",
        "    for name in tqdm(generic_names_list, desc=\"Fetching Brand Mappings\"):\n",
        "        processed_count += 1\n",
        "        if not isinstance(name, str) or not name.strip() or len(name.strip()) < 2:\n",
        "             continue #Skiping over invalid names\n",
        "\n",
        "        generic_name_cleaned = name.strip()\n",
        "        current_rxcui = get_rxcui(generic_name_cleaned, config)\n",
        "        time.sleep(config[\"api_delay_seconds\"]) #Delay\n",
        "\n",
        "        if current_rxcui:\n",
        "            brands = get_brand_names(current_rxcui, config)\n",
        "            time.sleep(config[\"api_delay_seconds\"]) #Delay\n",
        "\n",
        "            if brands:\n",
        "                selected_brand = brands[0].strip().lower()\n",
        "                if selected_brand:\n",
        "                    all_mappings.append({'generic_name': name, 'brand_name': selected_brand})\n",
        "                    found_count += 1\n",
        "\n",
        "    print(f\"API lookups completed and found {found_count}/{len(generic_names_list)} names\")\n",
        "    return all_mappings"
      ],
      "metadata": {
        "id": "qgECLNMHmuYa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 1: Fine-tuning**"
      ],
      "metadata": {
        "id": "N2r1rUUJ5bET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_classification = load_parquet_data(CONFIG[\"classification_data_path\"])\n",
        "if df_classification is not None:\n",
        "    tokenizer, model = load_tokenizer_and_model(CONFIG[\"base_model_name\"], CONFIG[\"num_labels\"])\n",
        "\n",
        "    if tokenizer and model:\n",
        "        train_ds, test_ds = prepare_hf_dataset(df_classification, tokenizer, CONFIG)\n",
        "\n",
        "        if train_ds and test_ds:\n",
        "            model = train_model(model, tokenizer, train_ds, test_ds, CONFIG)\n",
        "            if model:\n",
        "                print(\"Finetuning completed\")\n",
        "            else:\n",
        "                print(\"Model training failed\")\n",
        "        else:\n",
        "            print(\"Dataset preparation failed\")\n",
        "    else:\n",
        "        print(\"Base model or tokenizer loading failed\")\n",
        "else:\n",
        "    print(\"Classification data loading failed\")"
      ],
      "metadata": {
        "id": "QeUKd1wPxIIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 2: Drug Name Extraction and Saving**"
      ],
      "metadata": {
        "id": "zFAzKw-z5msl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_relation = load_parquet_data(CONFIG[\"relation_data_path\"])\n",
        "extracted_drugs = []\n",
        "if df_relation is not None:\n",
        "    extracted_drugs = extract_drug_names(df_relation)\n",
        "    if extracted_drugs:\n",
        "        save_json(extracted_drugs, CONFIG[\"extracted_drugs_file\"])\n",
        "        print(f\"Saved {len(extracted_drugs)} drug names to {CONFIG['extracted_drugs_file']}\")\n",
        "    else:\n",
        "        print(\"No drug names were extracted\")\n",
        "else:\n",
        "    print(\"Relation data loading failed\")"
      ],
      "metadata": {
        "id": "ChOJj2aSxLAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 3: Drug Masking Analysis (ADE Sentences)**"
      ],
      "metadata": {
        "id": "k_rQIQZS5pFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_classification' not in locals() or df_classification is None:\n",
        "    print(\"reloading classification data for masking analysis\")\n",
        "    df_classification = load_parquet_data(CONFIG[\"classification_data_path\"])\n",
        "\n",
        "if 'extracted_drugs' not in locals() or not extracted_drugs:\n",
        "     print(f\"Loading extracted drug names from {CONFIG['extracted_drugs_file']}\")\n",
        "     extracted_drugs = load_json(CONFIG[\"extracted_drugs_file\"])\n",
        "\n",
        "if 'model' not in locals() or 'tokenizer' not in locals() or model is None or tokenizer is None:\n",
        "    print(f\"loading finetuned model from {CONFIG['finetuned_model_dir']}...\")\n",
        "    tokenizer, model = load_tokenizer_and_model(CONFIG[\"finetuned_model_dir\"], CONFIG[\"num_labels\"])\n",
        "\n",
        "#Creating model wrapper or reusing\n",
        "if 'model_wrapper' not in locals() or model_wrapper is None:\n",
        "    model_wrapper = create_model_wrapper(model, tokenizer)\n",
        "\n",
        "\n",
        "if df_classification is not None and extracted_drugs and model_wrapper and tokenizer:\n",
        "    # Filter for ADE sentences\n",
        "    ade_df = df_classification[df_classification['label'] == CONFIG['ade_label']].copy()\n",
        "    print(f\"Processing {len(ade_df)} ADE sentences for masking analysis\")\n",
        "\n",
        "    if not ade_df.empty:\n",
        "        # Apply masking\n",
        "        mask_results = ade_df['text'].progress_apply(\n",
        "            lambda x: mask_drug_names(x, extracted_drugs, tokenizer.mask_token)\n",
        "        )\n",
        "        ade_df['masked_text'] = mask_results.apply(lambda x: x[0])\n",
        "        ade_df['drug_found_flag'] = mask_results.apply(lambda x: x[1])\n",
        "\n",
        "        # Filter df to rows where drug was actually found and masked\n",
        "        ade_df_masked = ade_df[ade_df['drug_found_flag'] == True].copy()\n",
        "        print(f\"Found drugs and applied masking to {len(ade_df_masked)} ADE sentences.\")\n",
        "\n",
        "        if not ade_df_masked.empty:\n",
        "             # Calculate flip rate\n",
        "             _, flip_examples, non_flip_examples = calculate_flip_rate(\n",
        "                  ade_df_masked, 'text', 'masked_text', model_wrapper, \"Masking (ADE Only)\"\n",
        "             )\n",
        "             # Save results\n",
        "             save_json(flip_examples, CONFIG[\"masking_flip_ade_file\"])\n",
        "             save_json(non_flip_examples, CONFIG[\"masking_nonflip_ade_file\"])\n",
        "             print(f\"Saved masking analysis examples for ADE sentences.\")\n",
        "        else:\n",
        "             print(\"No ADE sentences had detectable drug names from the list. Skipping flip rate calculation.\")\n",
        "    else:\n",
        "        print(\"No ADE sentences found in the dataset.\")\n",
        "else:\n",
        "    print(\"Skipping masking analysis due to missing data, drug names, or model.\")"
      ],
      "metadata": {
        "id": "4VCJmkrlxNGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 4: Drug Masking Analysis (Non ADE Sentences)**"
      ],
      "metadata": {
        "id": "_9i8OyIqND71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_classification' not in locals() or df_classification is None:\n",
        "    print(\"Reloading classification data for masking analysis...\")\n",
        "    df_classification = load_parquet_data(CONFIG[\"classification_data_path\"])\n",
        "\n",
        "if 'extracted_drugs' not in locals() or not extracted_drugs:\n",
        "     print(f\"Loading extracted drug names from {CONFIG['extracted_drugs_file']}\")\n",
        "     extracted_drugs = load_json(CONFIG[\"extracted_drugs_file\"])\n",
        "\n",
        "if 'model' not in locals() or 'tokenizer' not in locals() or model is None or tokenizer is None:\n",
        "    print(f\"Loading finetuned model from {CONFIG['finetuned_model_dir']}\")\n",
        "    tokenizer, model = load_tokenizer_and_model(CONFIG[\"finetuned_model_dir\"], CONFIG[\"num_labels\"])\n",
        "\n",
        "if 'model_wrapper' not in locals() or model_wrapper is None:\n",
        "    model_wrapper = create_model_wrapper(model, tokenizer)\n",
        "\n",
        "if \"masking_flip_non_ade_file\" not in CONFIG or \"masking_nonflip_non_ade_file\" not in CONFIG:\n",
        "    raise KeyError(\"Config Error\")\n",
        "\n",
        "if df_classification is not None and extracted_drugs and model_wrapper and tokenizer:\n",
        "    # Filter for Non-ADE sentences\n",
        "    non_ade_df = df_classification[df_classification['label'] != CONFIG['ade_label']].copy()\n",
        "    print(f\"Processing {len(non_ade_df)}\")\n",
        "\n",
        "    if not non_ade_df.empty:\n",
        "        mask_results = non_ade_df['text'].progress_apply(\n",
        "            lambda x: mask_drug_names(x, extracted_drugs, tokenizer.mask_token)\n",
        "        )\n",
        "        non_ade_df['masked_text'] = mask_results.apply(lambda x: x[0])\n",
        "        non_ade_df['drug_found_flag'] = mask_results.apply(lambda x: x[1])\n",
        "        non_ade_df_masked = non_ade_df[non_ade_df['drug_found_flag'] == True].copy()\n",
        "        print(f\"Found drugs and applied masking to {len(non_ade_df_masked)} Non-ADE sentences\")\n",
        "\n",
        "        if not non_ade_df_masked.empty:\n",
        "             # Calculate flip rate\n",
        "\n",
        "             _, flip_examples, non_flip_examples = calculate_flip_rate(\n",
        "                  non_ade_df_masked, 'text', 'masked_text', model_wrapper, \"Masking (Non-ADE Only)\"\n",
        "             )\n",
        "             # Save results\n",
        "             save_json(flip_examples, CONFIG[\"masking_flip_non_ade_file\"])\n",
        "             save_json(non_flip_examples, CONFIG[\"masking_nonflip_non_ade_file\"])\n",
        "             print(f\"Saved masking analysis examples for Non-ADE sentences.\")\n",
        "        else:\n",
        "             print(\"No drug names\")\n",
        "    else:\n",
        "        print(\"No Non-ADE sentences found in the dataset\")\n",
        "else:\n",
        "    print(\"error in masking analysis due to missing data\")"
      ],
      "metadata": {
        "id": "2JRi0oBWZ55n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 5: Fetch and Save Brand Name Mappings**"
      ],
      "metadata": {
        "id": "biEXRHac5sK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'extracted_drugs' not in locals() or not extracted_drugs:\n",
        "     print(f\"Loading extracted drug names from {CONFIG['extracted_drugs_file']}\")\n",
        "     extracted_drugs = load_json(CONFIG[\"extracted_drugs_file\"])\n",
        "\n",
        "brand_mappings_list = []\n",
        "if extracted_drugs:\n",
        "    brand_mappings_list = fetch_brand_mappings(extracted_drugs, CONFIG)\n",
        "    if brand_mappings_list:\n",
        "        save_json(brand_mappings_list, CONFIG[\"brand_mapping_file\"])\n",
        "        print(f\"Saved {len(brand_mappings_list)} brand mappings to {CONFIG['brand_mapping_file']}\")\n",
        "    else:\n",
        "        print(\"No brand mappings were etched\")\n",
        "else:\n",
        "    print(\"extracted drug list is missing\")"
      ],
      "metadata": {
        "id": "ms04NC-TxPKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 6: Apply Brand Name Replacement**"
      ],
      "metadata": {
        "id": "bcqlLHyw5vTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_classification' not in locals() or df_classification is None or 'text_brand_replaced' in df_classification.columns:\n",
        "    print(f\"Loading classification data from {CONFIG['classification_data_path']}\")\n",
        "    df_classification = load_parquet_data(CONFIG[\"classification_data_path\"])\n",
        "\n",
        "if 'brand_mappings_list' not in locals() or not brand_mappings_list:\n",
        "     print(f\"loading brand mappings from {CONFIG['brand_mapping_file']}\")\n",
        "     brand_mappings_list = load_json(CONFIG[\"brand_mapping_file\"])\n",
        "\n",
        "if df_classification is not None and brand_mappings_list:\n",
        "    brand_mapping_dict = {item['generic_name'].lower(): item['brand_name']\n",
        "                          for item in brand_mappings_list if 'generic_name' in item and 'brand_name' in item}\n",
        "\n",
        "    if brand_mapping_dict:\n",
        "        #Preparing for replacement\n",
        "        lower_map_dict, compiled_patterns, sorted_keys = prepare_brand_replacement(brand_mapping_dict)\n",
        "\n",
        "        print(\"Applying generic-to-brand replacement\")\n",
        "        #Applying replacement function\n",
        "        replacement_results = df_classification['text'].progress_apply(\n",
        "            lambda x: replace_generic_with_brand(x, lower_map_dict, compiled_patterns, sorted_keys)\n",
        "        )\n",
        "        df_classification['text_brand_replaced'] = replacement_results.apply(lambda x: x[0])\n",
        "        df_classification['brand_replaced_flag'] = replacement_results.apply(lambda x: x[1])\n",
        "        replaced_count = df_classification['brand_replaced_flag'].sum()\n",
        "        print(f\"{replaced_count} sentences had replacements\")\n",
        "        try:\n",
        "            check_and_create_dir(os.path.dirname(CONFIG[\"modified_data_file\"])) # Ensure dir exists\n",
        "            df_classification.to_parquet(CONFIG[\"modified_data_file\"], index=False)\n",
        "            print(f\"saved modified datato {CONFIG['modified_data_file']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"error saving modified data: {e}\")\n",
        "    else:\n",
        "        print(\"brand drug names dictionary is empty\")\n",
        "else:\n",
        "    print(\"missing data\")"
      ],
      "metadata": {
        "id": "b39LE5vQxRiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 7: Brand Replacement Flip Rate (All Sentences)**\n"
      ],
      "metadata": {
        "id": "t3yYkIQV5xwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_classification' not in locals() or 'brand_replaced_flag' not in df_classification.columns:\n",
        "     print(f\"Loading modified data from {CONFIG['modified_data_file']}\")\n",
        "     # Check if file exists before loading\n",
        "     if os.path.exists(CONFIG['modified_data_file']):\n",
        "         df_classification = load_parquet_data(CONFIG[\"modified_data_file\"])\n",
        "     else:\n",
        "         print(f\"data not found {CONFIG['modified_data_file']}\")\n",
        "         df_classification = None\n",
        "\n",
        "if 'model_wrapper' not in locals() or model_wrapper is None:\n",
        "    print(f\"Reloading fine-tuned model from {CONFIG['finetuned_model_dir']}...\")\n",
        "    tokenizer, model = load_tokenizer_and_model(CONFIG[\"finetuned_model_dir\"], CONFIG[\"num_labels\"])\n",
        "    model_wrapper = create_model_wrapper(model, tokenizer)\n",
        "\n",
        "\n",
        "if df_classification is not None and 'brand_replaced_flag' in df_classification.columns and model_wrapper:\n",
        "    # Filter for rows where replacement actually happened\n",
        "    df_replaced_all = df_classification[df_classification['brand_replaced_flag'] == True].copy()\n",
        "    print(f\"Analyzing {len(df_replaced_all)} sentences where brand replacement occurred.\")\n",
        "\n",
        "    if not df_replaced_all.empty:\n",
        "         # Calculate flip rate\n",
        "         _, flip_examples, non_flip_examples = calculate_flip_rate(\n",
        "              df_replaced_all, 'text', 'text_brand_replaced', model_wrapper, \"Brand Replace (All)\"\n",
        "         )\n",
        "         # Save results\n",
        "         save_json(flip_examples, CONFIG[\"brand_flip_all_file\"])\n",
        "         save_json(non_flip_examples, CONFIG[\"brand_nonflip_all_file\"])\n",
        "         print(f\"Saved brand replacement flip analysis\")\n",
        "    else:\n",
        "         print(\"No brand replacements\")\n",
        "else:\n",
        "    print(\"missing data\")"
      ],
      "metadata": {
        "id": "HXJi_eUwxUD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workflow - Step 8: Brand Replacement Flip Rate (ADE Sentences Only)**"
      ],
      "metadata": {
        "id": "hRyPxMEl56-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "required_cols = ['label', 'brand_replaced_flag', 'text', 'text_brand_replaced']\n",
        "if 'df_classification' not in locals() or not all(col in df_classification.columns for col in required_cols):\n",
        "     print(f\"Loading modified data from {CONFIG['modified_data_file']}\")\n",
        "     if os.path.exists(CONFIG['modified_data_file']):\n",
        "        df_classification = load_parquet_data(CONFIG[\"modified_data_file\"])\n",
        "     else:\n",
        "         print(f\"data not found {CONFIG['modified_data_file']}\")\n",
        "         df_classification = None\n",
        "\n",
        "if 'model_wrapper' not in locals() or model_wrapper is None:\n",
        "    print(f\"reloading finetuned model from {CONFIG['finetuned_model_dir']}\")\n",
        "    tokenizer, model = load_tokenizer_and_model(CONFIG[\"finetuned_model_dir\"], CONFIG[\"num_labels\"])\n",
        "    model_wrapper = create_model_wrapper(model, tokenizer)\n",
        "\n",
        "if df_classification is not None and all(col in df_classification.columns for col in required_cols) and model_wrapper:\n",
        "    df_replaced_ade = df_classification[\n",
        "        (df_classification['brand_replaced_flag'] == True) &\n",
        "        (df_classification['label'] == CONFIG['ade_label'])\n",
        "    ].copy()\n",
        "    print(f\"Analyzing {len(df_replaced_ade)} ADE sentences where brand replacement occurred.\")\n",
        "\n",
        "    if not df_replaced_ade.empty:\n",
        "        #calculating flip rate\n",
        "        _, flip_examples, non_flip_examples = calculate_flip_rate(\n",
        "            df_replaced_ade, 'text', 'text_brand_replaced', model_wrapper, \"Brand Replace (ADE Only)\"\n",
        "        )\n",
        "        save_json(flip_examples, CONFIG[\"brand_flip_ade_file\"])\n",
        "        save_json(non_flip_examples, CONFIG[\"brand_nonflip_ade_file\"])\n",
        "        print(f\"Saved brand replacement flip analysis examples (ADE sentences only).\")\n",
        "    else:\n",
        "        print(\"No ADE sentences found where brand replacement occurred. Skipping analysis.\")\n",
        "else:\n",
        "    if df_classification is None:\n",
        "         print(\"Skipping brand replacement flip analysis (ADE Only) because DataFrame is missing.\")\n",
        "    elif not all(col in df_classification.columns for col in required_cols):\n",
        "         print(f\"Skipping brand replacement flip analysis (ADE Only) because DataFrame is missing required columns: {required_cols}\")\n",
        "    elif model_wrapper is None:\n",
        "         print(\"Skipping brand replacement flip analysis (ADE Only) because Model Wrapper is missing.\")\n",
        "    else:\n",
        "         print(\"Skipping brand replacement flip analysis (ADE Only) due to missing data or model.\")"
      ],
      "metadata": {
        "id": "FSmUnCr3xWJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}